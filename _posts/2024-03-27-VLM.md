---
layout: post
title: Vision Language Models
author: [Richard Kuo]
category: [Lecture]
tags: [jekyll, ai]
---

Introduction to VLMs, MM-LLMs.

---
## [Guide to Vision-Language Models (VLMs)](https://encord.com/blog/vision-language-models-guide)
**[LLM in Vision papers](https://github.com/DirtyHarryLYL/LLM-in-Vision)**<br>

### Contrastive Learning
**CLIP architecture**<br>
![](https://images.prismic.io/encord/04311c42-2635-40fb-9187-2847b87224a7_image9.png?auto=compress,format)

---
### PrefixLM
**SimVLM architecture**<br>
![](https://images.prismic.io/encord/80a849e3-8867-4414-bde0-da76df2314de_image3.png?auto=compress,format)

**VirTex architecture**<br>
![](https://images.prismic.io/encord/39a495c5-b563-4253-8722-c58dda94eeb3_image2.png?auto=compress,format)

**Frozen architecture**<br>
![](https://images.prismic.io/encord/65b4b1ac-6af0-41e2-9ac7-e7cdbd67a589_image10.png?auto=compress,format)

**Flamingo architecture**<br>
![](https://images.prismic.io/encord/b94edb70-7caf-4944-9836-2d58dad8b91a_image4.png?auto=compress,format)

---
### Multimodal Fusing with Cross-Attention
**VisualGPT architecture**<br>
![](https://images.prismic.io/encord/90aecd3e-cb7d-4df9-b0df-a64c1a9f9a9d_image6.png?auto=compress,format)

---
### Masked-language Modeling (MLM) & Image-Text Matching (ITM)
**VisualBERT architecture**<br>
![](https://images.prismic.io/encord/9e2fdf0f-24ca-4bf6-9df2-712da43be018_image12.png?auto=compress,format)

---
## MM-LLMs
**Paper:** [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/html/2401.13601v1)<br>
![](https://arxiv.org/html/2401.13601v1/x1.png)

**The general model architecture of MM-LLMs**<br>
![](https://arxiv.org/html/2401.13601v1/x2.png)

---
### [Next-GPT](https://next-gpt.github.io/)
**Paper:** [Any-to-Any Multimodal Large Language Model](https://arxiv.org/abs/2309.05519)<br>
![](https://next-gpt.github.io/static/images/framework.png)

---
### Ferret
**Paper:** [Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://arxiv.org/abs/2310.07704)<br>
**Code:** [https://github.com/apple/ml-ferret](https://github.com/apple/ml-ferret)<br>
![](https://github.com/apple/ml-ferret/raw/main/figs/ferret_fig_diagram_v2.png)

---
### MiniGPT-v2
**Paper:** [MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning](https://arxiv.org/abs/2310.09478)<br>
**Code:** [https://github.com/Vision-CAIR/MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)<br>
![](https://github.com/Vision-CAIR/MiniGPT-4/raw/main/figs/minigpt2_demo.png)

---
### GPT4-V
**Paper:** [Assessing GPT4-V on Structured Reasoning Tasks](https://arxiv.org/abs/2312.11524)<br>

---
### Gemini
**Paper:** [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)<br>
![](https://github.com/rkuo2000/AI-course/blob/main/images/Gemini.png?raw=true)

---
### [PaLM-E](https://palm-e.github.io/)
**Paper:** [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)<br>
**Code:** [https://github.com/kyegomez/PALM-E](https://github.com/kyegomez/PALM-E)<br>
![](https://github.com/kyegomez/PALM-E/raw/main/image6.png)

---
### PaLI-X
**Paper:** [PaLI-X: On Scaling up a Multilingual Vision and Language Model](https://arxiv.org/abs/2305.18565)<br>

---
### Qwen-VL
**model:** [Qwen/Qwen-VL-Chat](https://huggingface.co/Qwen/Qwen-VL-Chat)<br>
**Paper:** [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)<br>
**Code:** [https://github.com/QwenLM/Qwen-VL](https://github.com/QwenLM/Qwen-VL)<br>

---
### Yi-VL-34B
**model:** [01-ai/Yi-VL-34B](https://huggingface.co/01-ai/Yi-VL-34B)<br>

---
### VILA
**Paper:** [VILA: On Pre-training for Visual Language Models](https://arxiv.org/abs/2312.07533)<br>
**Github:** [https://github.com/Efficient-Large-Model/VILA](https://github.com/Efficient-Large-Model/VILA)<br>
<video control><src="https://private-user-images.githubusercontent.com/7783214/307362402-6079374c-0787-4bc4-b9c6-e1524b4c9dc4.mp4"></video>

---
### [LLaVA](https://llava-vl.github.io/)
**Paper:** [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)<br>
**Paper:** [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)<br>
**Code:** [https://github.com/haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA)<br>
![](https://github.com/rkuo2000/GenAI/blob/main/assets/LLaVA_Gradio_Server_UI.png?raw=true)

---
### FuYu-8B
**Blog:** [Fuyu-8B: A Multimodal Architecture for AI Agents](https://www.adept.ai/blog/fuyu-8b)<br>
![](https://www.adept.ai/images/blog/fuyu-8b/architecture.png)
![](https://github.com/rkuo2000/AI-course/blob/main/images/VLMs_benchmark.png?raw=true)

---
### [VLFeedback and Silkie](https://vlf-silkie.github.io/)
**Paper:** [Silkie: Preference Distillation for Large Visual Language Models](https://arxiv.org/abs/2312.10665)<br>
**Code:** [https://github.com/vlf-silkie/VLFeedback](https://github.com/vlf-silkie/VLFeedback)<br>

---
### [MyVLM](https://snap-research.github.io/MyVLM/)
**Paper:** [MyVLM: Personalizing VLMs for User-Specific Queries](https://arxiv.org/abs/2403.14599)<br>
**Code:** [https://github.com/snap-research/MyVLM](https://github.com/snap-research/MyVLM)<br>
![](https://github.com/snap-research/MyVLM/raw/master/docs/teaser.jpg)


<br>
<br>

*This site was last updated {{ site.time | date: "%B %d, %Y" }}.*

